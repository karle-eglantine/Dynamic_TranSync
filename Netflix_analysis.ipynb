{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netflix dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "\n",
    "import sys\n",
    "sys.path.append('modules')\n",
    "\n",
    "import netflix_module as netflix\n",
    "import graph_module as graph\n",
    "import loocv_module as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '/Users/eglantine.karle/Docs/These/Ranking/LS approach/Code/Revision/netflix_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data (run only the first time to create data files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(dir_path+'data.csv'):\n",
    "    # Create a file 'data.csv' before reading it\n",
    "    # Read all the files in the dataset and store them in one big file ('data.csv')\n",
    "    # We're reading from each of the four files and appending each rating to a global file 'data.csv'\n",
    "    data = open(dir_path+'data.csv', mode='w')\n",
    "    \n",
    "    row = list()\n",
    "    files = [\n",
    "        dir_path+'combined_data_1.txt',\n",
    "        dir_path+'combined_data_2.txt', \n",
    "        dir_path+'combined_data_3.txt', \n",
    "        dir_path+'combined_data_4.txt'\n",
    "    ]\n",
    "    for file in files:\n",
    "        print(\"Reading ratings from {}\\n\".format(file))\n",
    "        with open(file) as f:\n",
    "            for line in f: \n",
    "                line = line.strip()\n",
    "                if line.endswith(':'):\n",
    "                    # All below are ratings for this movie, until another movie appears.\n",
    "                    movie_id = line.replace(':', '')\n",
    "                else:\n",
    "                    row = [x for x in line.split(',')]\n",
    "                    row.insert(0, movie_id)\n",
    "                    data.write(','.join(row))\n",
    "                    data.write('\\n')\n",
    "    data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating the dataframe from data.csv file\")\n",
    "df = pd.read_csv(dir_path+'data.csv', sep=',', \n",
    "    names=['movie', 'user', 'rating', 'date'])\n",
    "df.date = pd.to_datetime(df.date)\n",
    "df.date = df['date'].dt.to_period('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are arranging the ratings according to time-stamp(s)\n",
    "print('Sorting the dataframe by Date')\n",
    "df.sort_values(by='date', inplace=True)\n",
    "\n",
    "#df.to_csv('netflix_data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df.date.unique()\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id = pd.read_csv(dir_path+'movie_titles.csv',sep=';',header=None)\n",
    "movie_id = movie_id.iloc[:,[0,1]]\n",
    "movie_id.columns = ['Year','Movie'] \n",
    "movie_id.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset with $N=100$ movies (run only the first time to create data files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_100 = pd.read_csv('netflix_data/titles_100_movies.csv',sep=';')\n",
    "titles_100.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_movies = [i+1 for i in titles_100.Id.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of dates to merge in order to get all connected graphs\n",
    "\n",
    "N = 100\n",
    "list_A = [np.zeros((N,N))]\n",
    "df_100 = df.loc[df.movie.isin(l_movies)]\n",
    "\n",
    "k = 0\n",
    "d = []\n",
    "merged_dates = [dates[0]]\n",
    "while k< len(dates):\n",
    "    if graph.connected(list_A[-1]):\n",
    "        # if the last graph is connected, we start a new graph using the data of the current graph\n",
    "        A = np.zeros((N,N))\n",
    "        d = [dates[k]]\n",
    "        df_d = df_100.loc[df_100.date.isin(d)] # Dataset at time d\n",
    "        l_movies_d = df_d.movie.unique() # list of rated movies at time d\n",
    "        for [i,j] in combinations(l_movies_d,2):\n",
    "            a = l_movies.index(i)\n",
    "            b = l_movies.index(j)\n",
    "\n",
    "            A[a,b] = 1\n",
    "            A[b,a] = 1\n",
    "        list_A.append(A)\n",
    "        merged_dates.append(d)\n",
    "            \n",
    "    else:\n",
    "        # if the last graph is not connected, we add the data of the current graph\n",
    "        d.append(dates[k])\n",
    "        A = np.zeros((N,N))\n",
    "        df_d = df_100.loc[df_100.date.isin(d)] # Dataset at time d\n",
    "        l_movies_d = df_d.movie.unique() # list of rated movies at time d\n",
    "        for [i,j] in combinations(l_movies_d,2):\n",
    "            a = l_movies.index(i)\n",
    "            b = l_movies.index(j)\n",
    "\n",
    "            A[a,b] = 1\n",
    "            A[b,a] = 1\n",
    "        list_A[-1] = A\n",
    "        merged_dates[-1] = d\n",
    "        \n",
    "    k = k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "T = len(merged_dates)\n",
    "Y = np.zeros((T,N,N))\n",
    "A = np.zeros((T,N,N))\n",
    "df_100 = df.loc[df.movie.isin(l_movies)]\n",
    "for k,d in enumerate(merged_dates):\n",
    "    df_d = df_100.loc[df_100.date.isin(d)] # Dataset at time d\n",
    "    l_movies_d = df_d.movie.unique() # list of rated movies at time d\n",
    "    for [i,j] in combinations(l_movies_d,2):\n",
    "        a = l_movies.index(i)\n",
    "        b = l_movies.index(j)\n",
    "        \n",
    "        # Mean score for each movie at time d\n",
    "        rating_i = np.mean(df_d['rating'].values[df_d.movie == i])\n",
    "        rating_j = np.mean(df_d['rating'].values[df_d.movie == j])\n",
    "        Y[k,a,b] = rating_i-rating_j\n",
    "        Y[k,b,a] = -Y[k,a,b]\n",
    "        A[k,a,b] = 1 # Movies i and j were compared at time t so we add an edge on the comparison graph\n",
    "        A[k,b,a] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"netflix_data/y_merged_transync_100.txt\", \"wb\") as y:\n",
    "    pickle.dump(Y, y)\n",
    "with open(\"netflix_data/a_merged_transync_100.txt\", \"wb\") as a:\n",
    "    pickle.dump(A, a)\n",
    "with open(\"netflix_data/100_movies.txt\", \"wb\") as m:\n",
    "    pickle.dump(l_movies, m)\n",
    "with open(\"netflix_data/merged_dates_100.txt\", \"wb\") as mer:\n",
    "    pickle.dump(merged_dates, mer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis - 100 movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(dir_path+'y_merged_transync_100.txt', \"rb\") as y:\n",
    "    Y = pickle.load(y)\n",
    "with open(dir_path+'a_merged_transync_100.txt', \"rb\") as a:\n",
    "    A = pickle.load(a)\n",
    "with open(dir_path+'100_movies.txt', \"rb\") as m:\n",
    "    movies = pickle.load(m)\n",
    "with open(dir_path+\"merged_dates_100.txt\", \"rb\") as mer:\n",
    "    merged_dates = pickle.load(mer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = pd.read_csv(dir_path+'titles_100_movies.csv',sep=';')\n",
    "df_movies = pd.DataFrame({'Movies': titles.Title.loc[titles.Id.isin([m-1 for m in movies])]})\n",
    "df_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T,N = np.shape(Y)[:2]\n",
    "T,N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graph_module as graph\n",
    "print(graph.connected(sum(A)))\n",
    "for t in range(T):\n",
    "    print(graph.connected(A[t,:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the union graph is connected and all the merged graphs are connected.\n",
    "We've reduced the number of timepoints to 23 to obtain this connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity = []\n",
    "for t in range(T):\n",
    "    sparsity.append(1.0 - ( np.count_nonzero(A[t,:,:]) / float(A[t,:,:].size) ))\n",
    "sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observed graph here are dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loocv_module as cv\n",
    "import ls_module as ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = ls.penalty_E(N,T)\n",
    "with open(\"/Users/eglantine.karle/Docs/These/Ranking/LS approach/Code/Revision/eigenpairs_E/eigenvectors_E_N\"+str(N)+\"_T\"+str(T)+\".txt\", \"rb\") as v:\n",
    "        V_E = pickle.load(v)\n",
    "with open(\"/Users/eglantine.karle/Docs/These/Ranking/LS approach/Code/Revision/eigenpairs_E/eigenvalues_E_N\"+str(N)+\"_T\"+str(T)+\".txt\", \"rb\") as e:\n",
    "        eigs_E = pickle.load(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation for Upsets criterion\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "lambda_list_up = np.linspace(0,0.2,20)\n",
    "tau_list_up = np.linspace(370,410,20)\n",
    "\n",
    "result_dls_up = cv.cv_dls_transync_up(Y,A,E,lambda_list_up,num_loocv = 40)\n",
    "result_dproj_up = cv.cv_dproj_transync_up(Y,A,E,V_E,eigs_E,tau_list_up,num_loocv = 40)\n",
    "lam_up_dls,z_up_dls,error_up_dls = result_dls_up\n",
    "tau_up_dproj,z_up_dproj,error_up_dproj = result_dproj_up\n",
    "print(lam_up_dls,tau_up_dproj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation for MSE criterion\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "lambda_list_mse = np.linspace(0,500,20)\n",
    "tau_list_mse = np.linspace(1e-6,1,20)\n",
    "\n",
    "result_dls_mse = cv.cv_dls_transync_mse(Y,A,E,lambda_list_mse,num_loocv = 40)\n",
    "result_dproj_mse = cv.cv_dproj_transync_mse(Y,A,E,V_E,eigs_E,tau_list_mse,num_loocv = 40)\n",
    "lam_mse_dls,z_mse_dls,error_mse_dls = result_dls_mse\n",
    "tau_mse_dproj,z_mse_dproj,error_mse_dproj = result_dproj_mse\n",
    "print(lam_mse_dls,tau_mse_dproj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam_mse_dls,tau_mse_dproj = [421.05263157894734,0.2631586315789473]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis for parameter $\\tau^*$ and $\\lambda^*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_merged_dates = [d[0] for d in merged_dates]\n",
    "df_movies = pd.DataFrame({'Movies': titles.Title.loc[titles.Id.isin([m-1 for m in movies])]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_ls_mse,l_dls_mse,l_dproj_mse = netflix.get_ranks_transync(lam_mse_dls,tau_mse_dproj,Y,A,movies,titles,first_merged_dates,\n",
    "                                                            ls_flag = True,dls_flag = True,dproj_flag = True)\n",
    "l_ls_up,l_dls_up,l_dproj_up = netflix.get_ranks_transync(lam_up_dls,tau_up_dproj,Y,A,movies,titles,first_merged_dates,\n",
    "                                                            ls_flag = True,dls_flag = True,dproj_flag = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Results for MSE criteria\n",
    "z_dls_mse,df_dls_mse,lam_mse = l_dls_mse\n",
    "z_ls_mse,df_ls_mse = l_ls_mse\n",
    "z_dproj_mse,df_dproj_mse,tau_mse = l_dproj_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Results for Upsets criteria\n",
    "z_dls_up,df_dls_up,lam_up = l_dls_up\n",
    "z_ls_up,df_ls_up = l_ls_up\n",
    "z_dproj_up,df_dproj_up,tau_up = l_dproj_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of upsets for upsets results\n",
    "upsets_ls_up = netflix.get_mean_upsets_transync(Y,A,z_ls_up)\n",
    "upsets_dls_up = netflix.get_mean_upsets_transync(Y,A,z_dls_up)\n",
    "upsets_dproj_up = netflix.get_mean_upsets_transync(Y,A,z_dproj_up)\n",
    "\n",
    "# MSE for upsets results\n",
    "mse_ls_up = netflix.get_mse_upsets_transync(Y,A,z_ls_up)\n",
    "mse_dls_up = netflix.get_mse_upsets_transync(Y,A,z_dls_up)\n",
    "mse_dproj_up = netflix.get_mse_upsets_transync(Y,A,z_dproj_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of upsets for mse results\n",
    "upsets_ls_mse = netflix.get_mean_upsets_transync(Y,A,z_ls_mse)\n",
    "upsets_dls_mse = netflix.get_mean_upsets_transync(Y,A,z_dls_mse)\n",
    "upsets_dproj_mse = netflix.get_mean_upsets_transync(Y,A,z_dproj_mse)\n",
    "\n",
    "# MSE for mse results\n",
    "mse_ls_mse = netflix.get_mse_upsets_transync(Y,A,z_ls_mse)\n",
    "mse_dls_mse = netflix.get_mse_upsets_transync(Y,A,z_dls_mse)\n",
    "mse_dproj_mse = netflix.get_mse_upsets_transync(Y,A,z_dproj_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean number of upsets for each method\n",
    "np.mean(upsets_ls_up),np.mean(upsets_dls_up),np.mean(upsets_dproj_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean MSE for each method\n",
    "np.mean(mse_ls_mse),np.mean(mse_dls_mse),np.mean(mse_dproj_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another performance criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check smoothness of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the ground truth vector $z^*$ such that \n",
    "$$z^*_{t,i} = \\frac{1}{N_{t,i}} \\sum_{j \\in N_{t,i}} y_{ij}(t)$$\n",
    "where $N_{t,i}$ denotes the set of neighbours of $i$ at time $t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ground truth\n",
    "z_star = np.zeros((T,N))\n",
    "for t in range(T):\n",
    "    for i in range(N):\n",
    "        Nti = np.sum(A[t,i,:]) # Number of games played by team i at time t\n",
    "        if Nti != 0:\n",
    "            z_star[t,i] = np.sum(Y[t,i,:])/Nti\n",
    "        else:\n",
    "            z_star[t,i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "for i in range(5):\n",
    "    ax.plot(z_star[:,i],label=df_movies.iloc[i]['Movies'])\n",
    "ax.set_ylabel('Ground truth $z^{*,emp}_{t,i}$')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_title('Evolution of the strength of movies')\n",
    "plt.legend(loc='best',frameon=False)\n",
    "\n",
    "fig.savefig(res_path+'smoothness_netflix_5films.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
