{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook, written by E. Karl√©, contains the code necessary to reproduce Table 1 and Figure 6a from the article Dynamic Ranking and Translation Synchronization https://arxiv.org/pdf/2207.01455.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "\n",
    "import sys\n",
    "sys.path.append('modules')\n",
    "\n",
    "import real_data_module as rdata\n",
    "import graph_module as graph\n",
    "import loocv_module as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this notebook, the user can download the data from http://www.netflixprize.com, which is provided as 4 txt files and one csv file containing names of the movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder where the data is stored\n",
    "\n",
    "dir_path = '/Users/eglantine.karle/Docs/These/Ranking/LS approach/Code/Revision/netflix_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the code needs to be run the first time one uses the notebook. The prepared data is then automatically saved in the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combination of all data into one csv file\n",
    "if not os.path.isfile(dir_path+'data.csv'):\n",
    "    # Create a file 'data.csv' before reading it\n",
    "    # Read all the files in the dataset and store them in one big file ('data.csv')\n",
    "    # We're reading from each of the four files and appending each rating to a global file 'data.csv'\n",
    "    data = open(dir_path+'data.csv', mode='w')\n",
    "    \n",
    "    row = list()\n",
    "    files = [\n",
    "        dir_path+'combined_data_1.txt',\n",
    "        dir_path+'combined_data_2.txt', \n",
    "        dir_path+'combined_data_3.txt', \n",
    "        dir_path+'combined_data_4.txt'\n",
    "    ]\n",
    "    for file in files:\n",
    "        print(\"Reading ratings from {}\\n\".format(file))\n",
    "        with open(file) as f:\n",
    "            for line in f: \n",
    "                line = line.strip()\n",
    "                if line.endswith(':'):\n",
    "                    # All below are ratings for this movie, until another movie appears.\n",
    "                    movie_id = line.replace(':', '')\n",
    "                else:\n",
    "                    row = [x for x in line.split(',')]\n",
    "                    row.insert(0, movie_id)\n",
    "                    data.write(','.join(row))\n",
    "                    data.write('\\n')\n",
    "    data.close()\n",
    "    \n",
    "# Creating the dataframe from data.csv file\n",
    "df = pd.read_csv(dir_path+'data.csv', sep=',', \n",
    "    names=['movie', 'user', 'rating', 'date'])\n",
    "df.date = pd.to_datetime(df.date)\n",
    "df.date = df['date'].dt.to_period('M')\n",
    "\n",
    "# Arranging the ratings according to time-stamp(s)\n",
    "df.sort_values(by='date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the time stamps\n",
    "dates = df.date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003.0</td>\n",
       "      <td>Dinosaur Planet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004.0</td>\n",
       "      <td>Isle of Man TT 2004 Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997.0</td>\n",
       "      <td>Character</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>Paula Abdul's Get Up &amp; Dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004.0</td>\n",
       "      <td>The Rise and Fall of ECW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year                         Movie\n",
       "0  2003.0               Dinosaur Planet\n",
       "1  2004.0    Isle of Man TT 2004 Review\n",
       "2  1997.0                     Character\n",
       "3  1994.0  Paula Abdul's Get Up & Dance\n",
       "4  2004.0      The Rise and Fall of ECW"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all the movie ids\n",
    "movie_id = pd.read_csv(dir_path+'movie_titles.csv',sep=';',header=None)\n",
    "movie_id = movie_id.iloc[:,[0,1]]\n",
    "movie_id.columns = ['Year','Movie'] \n",
    "movie_id.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of $N=100$ movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset contains 17770 movies. For computational reasons, we extract a subset of 100 movies. We select first the 25 movies that have been rated at all of the 74 time points. We then add 75 random other movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection of 100 movies\n",
    "\n",
    "df_rated_movies = df.loc[df.date == '1999-11'] # Movies rated at the first time point\n",
    "l_movies = []\n",
    "for m in df_rated_movies.movie.unique():\n",
    "    df_movie = df.loc[df.movie == m]\n",
    "    if len(df_movie.date.unique()) == 74: # Add movies observed at all times\n",
    "        l_movies.append(m)\n",
    "        \n",
    "for m in df.movie.unique(): # Select other random movies to our list\n",
    "    if len(l_movies)<100:\n",
    "        if m not in l_movies:\n",
    "            l_movies.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of dates to merge in order to get all connected graphs\n",
    "# We gather data chronologically into graphs. We start a new graph as soon as the previous graph is connected\n",
    "\n",
    "\n",
    "N = 100\n",
    "list_A = [np.zeros((N,N))] # list of adjacency matrix\n",
    "df_100 = df.loc[df.movie.isin(l_movies)] # df of the selected 100 movies\n",
    "\n",
    "k = 0\n",
    "d = []\n",
    "merged_dates = [dates[0]] # list of merged time points to form connected graphs\n",
    "while k< len(dates):\n",
    "    if graph.connected(list_A[-1]):\n",
    "        # if the last graph is connected, we start a new graph using the data of the current graph\n",
    "        A = np.zeros((N,N))\n",
    "        d = [dates[k]] # time points used to form the current graph\n",
    "        df_d = df_100.loc[df_100.date.isin(d)] # Dataset at time d\n",
    "        l_movies_d = df_d.movie.unique() # list of rated movies at time d\n",
    "        \n",
    "        # Add 1 in adjacency matrix for the movies rated at time d \n",
    "        for [i,j] in combinations(l_movies_d,2):\n",
    "            a = l_movies.index(i)\n",
    "            b = l_movies.index(j)\n",
    "            A[a,b] = 1\n",
    "            A[b,a] = 1\n",
    "        list_A.append(A)\n",
    "        merged_dates.append(d)\n",
    "            \n",
    "    else:\n",
    "        # if the last graph is not connected, we add the data of the current graph\n",
    "        d.append(dates[k]) # time points used for this current graph\n",
    "        A = np.zeros((N,N))\n",
    "        df_d = df_100.loc[df_100.date.isin(d)] # Dataset at time d\n",
    "        l_movies_d = df_d.movie.unique() # list of rated movies at time d\n",
    "        # Add 1 in adjacency matrix for the movies rated at time d \n",
    "        for [i,j] in combinations(l_movies_d,2):\n",
    "            a = l_movies.index(i)\n",
    "            b = l_movies.index(j)\n",
    "            A[a,b] = 1\n",
    "            A[b,a] = 1\n",
    "        list_A[-1] = A\n",
    "        merged_dates[-1] = d\n",
    "        \n",
    "    k = k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the observation and adjacency matrix from the merged dates\n",
    "\n",
    "N = 100 # Number of movies\n",
    "T = len(merged_dates) # Number of time points (number of observed connected graphs)\n",
    "Y = np.zeros((T,N,N))\n",
    "A = np.zeros((T,N,N))\n",
    "df_100 = df.loc[df.movie.isin(l_movies)]\n",
    "for k,d in enumerate(merged_dates):\n",
    "    df_d = df_100.loc[df_100.date.isin(d)] # Dataset at time d\n",
    "    l_movies_d = df_d.movie.unique() # list of rated movies at time d\n",
    "    for [i,j] in combinations(l_movies_d,2):\n",
    "        a = l_movies.index(i)\n",
    "        b = l_movies.index(j)\n",
    "        \n",
    "        # Mean score for each movie at time d\n",
    "        rating_i = np.mean(df_d['rating'].values[df_d.movie == i])\n",
    "        rating_j = np.mean(df_d['rating'].values[df_d.movie == j])\n",
    "        # Update the observation matrix with the mean rating difference\n",
    "        Y[k,a,b] = rating_i-rating_j\n",
    "        Y[k,b,a] = -Y[k,a,b]\n",
    "        # Update the adjacency matrix\n",
    "        A[k,a,b] = 1 \n",
    "        A[k,b,a] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the prepared data\n",
    "\n",
    "import pickle\n",
    "with open(\"netflix_data/y_merged_transync_100.txt\", \"wb\") as y:\n",
    "    pickle.dump(Y, y)\n",
    "with open(\"netflix_data/a_merged_transync_100.txt\", \"wb\") as a:\n",
    "    pickle.dump(A, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of 100 movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been prepared once, the user can directly proceed with the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(dir_path+'y_merged_transync_100.txt', \"rb\") as y:\n",
    "    Y = pickle.load(y) # Observation matrix\n",
    "with open(dir_path+'a_merged_transync_100.txt', \"rb\") as a:\n",
    "    A = pickle.load(a) # Adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T,N = np.shape(Y)[:2] # Number of time points and of movies\n",
    "T,N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, we verify that the union graph and all the individual graphs are connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graph_module as graph\n",
    "print(graph.connected(sum(A)))\n",
    "for t in range(T):\n",
    "    print(graph.connected(A[t,:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the union graph is connected and all the merged graphs are connected.\n",
    "We've reduced the number of timepoints to 23 to obtain this connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sparsity of the graphs\n",
    "\n",
    "sparsity = []\n",
    "for t in range(T):\n",
    "    sparsity.append(1.0 - ( np.count_nonzero(A[t,:,:]) / float(A[t,:,:].size) ))\n",
    "sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observed graph here are dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis via DLS and DProj method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loocv_module as cv\n",
    "import tools_module as tools\n",
    "import smoothness_module as smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothness parameters used for the analysis\n",
    "\n",
    "E = smooth.penalty_E(N,T)\n",
    "eigs_E,V_E = smooth.eigs_E(N,T-1) # eigs_E compute the eigs of the smoothness operator for T+1 graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of $\\lambda^*$ and $\\tau^*$ through cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run two cross-validations procedures in order to get optimal values for the hyper parameter $\\lambda$ and $\\tau$. The criteria for these procedures are the MSE and the mean number of upsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation with the number of upsets criterion\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "lambda_list_up = np.linspace(0,0.2,20) # Candidates for lambda\n",
    "tau_list_up = np.linspace(370,410,20) # Candidates for tau\n",
    "\n",
    "# Analysis with the DLS method\n",
    "lam_up,z_up_dls = cv.cv_dls_up(Y,A,E,lambda_list_up,num_loocv = 40)\n",
    "\n",
    "# Analysis with the DProj method\n",
    "tau_up,z_up_dproj = cv.cv_dproj_up(Y,A,V_E,eigs_E,tau_list_up,num_loocv = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation with MSE criterion\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "lambda_list_mse = np.linspace(0,500,20) # Candidates for lambda\n",
    "tau_list_mse = np.linspace(1e-6,1,20) # Candidates for tau\n",
    "\n",
    "# Analysis with the DLS method\n",
    "lam_mse,z_mse_dls = cv.cv_dls_mse(Y,A,E,lambda_list_mse,num_loocv = 40)\n",
    "\n",
    "# Analysis with the DProj method\n",
    "tau_mse,z_mse_dproj = cv.cv_dproj_mse(Y,A,V_E,eigs_E,tau_list_mse,num_loocv = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lam_up,lam_mse,tau_up,tau_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation procedures give estimators for the optimal value of hyperparameters in DLS and DProj method. Let us compute the naive LS estimator for the sake of comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computation of the LS estimator\n",
    "\n",
    "Y_vec = tools.obs_transync(Y,A) # Vectorize the observations\n",
    "Q = graph.diag_incidence(A)\n",
    "Lv = Q@Q.T # Laplacian matrix\n",
    "z_ls = scipy.sparse.linalg.lsqr(Lv,Q@Y_vec)[0] # LS estimator\n",
    "z_ls = z_ls.reshape((T,N),order='F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compute the error criterion for each estimator using the observations as ground truth. \n",
    "For estimators obtained by cross-validation with the Upsets criterion, we compute the mean number of upsets with respect to the observations.\n",
    "For estimators obtained by cross-validation with the MSE criterion, we compute the MSE with respect to the observed score differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Number of upsets \n",
    "upsets_ls = rdata.get_mean_nb_upsets(Y,A,z_ls)\n",
    "upsets_dls = rdata.get_mean_nb_upsets(Y,A,z_up_dls)\n",
    "upsets_dproj = rdata.get_mean_nb_upsets(Y,A,z_up_dproj)\n",
    "\n",
    "print(upsets_ls,upsets_dls,upsets_dproj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE\n",
    "MSE_ls = rdata.get_mse(Y,A,z_ls)\n",
    "MSE_dls = rdata.get_mse(Y,A,z_mse_dls)\n",
    "MSE_dproj = rdata.get_mse(Y,A,z_mse_dproj)\n",
    "\n",
    "print(MSE_ls,MSE_dls,MSE_dproj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check : smoothness of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our analysis rely on a supposed smoothness of the data. Let us check that this dataset fits this criteria by defining a ground truth vector from the observations and plot its evolution for some movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the ground truth vector $z^*$ such that \n",
    "$$z^*_{t,i} = \\frac{1}{N_{t,i}} \\sum_{j \\in N_{t,i}} y_{ij}(t)$$\n",
    "where $N_{t,i}$ denotes the set of neighbours of $i$ at time $t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ground truth\n",
    "z_star = np.zeros((T,N))\n",
    "for t in range(T):\n",
    "    for i in range(N):\n",
    "        Nti = np.sum(A[t,i,:]) # Number of games played by team i at time t\n",
    "        if Nti != 0:\n",
    "            z_star[t,i] = np.sum(Y[t,i,:])/Nti\n",
    "        else:\n",
    "            z_star[t,i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for 5 movies\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "for i in range(5):\n",
    "    ax.plot(z_star[:,i],label=df_movies.iloc[i]['Movies'])\n",
    "ax.set_ylabel('Ground truth $z^{*,emp}_{t,i}$')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_title('Evolution of the strength of movies')\n",
    "plt.legend(loc='best',frameon=False)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
