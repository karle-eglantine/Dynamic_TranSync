{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook, written by E. Karl√©, contains the code necessary to reproduce Table 2 and Figure 6b from the article Dynamic Ranking and Translation Synchronization https://arxiv.org/pdf/2207.01455.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "\n",
    "import sys\n",
    "sys.path.append('modules')\n",
    "\n",
    "import real_data_module as rdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this notebook, the user can download the data from https://www.kaggle.com/datasets/saife245/english-premier-league, which is provided as csv files for each season. Note that this dataset is regularly kept up to date but this notebook only uses data from the seasons 2000-2001 to 2017-2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder where the data is saved\n",
    "\n",
    "dir_path = 'epl_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the code needs to be run the first time one uses the notebook. The prepared data is then automatically saved in the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of one dataset with all the results from seasons between 2000 and 2017 \n",
    "all_seasons = np.arange(2000,2018)\n",
    "\n",
    "# Initialize the data frame with the first season\n",
    "df = pd.read_csv(dir_path+'2000.csv', sep=';',index_col=0)\n",
    "df['Season'] = 2000\n",
    "data = df.loc[:,['Season','MW','HomeTeam','AwayTeam','FTHG','FTAG']]\n",
    "data = data.rename(columns={'MW':'Week'})\n",
    "\n",
    "for s in all_seasons[1:]:\n",
    "    df = pd.read_csv(dir_path+str(s)+'.csv', sep=';',index_col=0)\n",
    "    df['Season'] = s\n",
    "    df = df.loc[:,['Season','MW','HomeTeam','AwayTeam','FTHG','FTAG']]\n",
    "    df = df.rename(columns={'MW':'Week'})\n",
    "    data = pd.concat([data, df],ignore_index = True,sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computation of score differences\n",
    "data['Score'] = data.FTHG-data.FTAG\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get names of all teams that played in EPL between 2000 and 2017\n",
    "teams = np.union1d(data.HomeTeam.unique().astype(str),data.AwayTeam.unique().astype(str))\n",
    "N = len(teams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no subsets of succesive seasons that form a connected graph. Hence we will merge data manually by group of 2 seasons in order to denoise the observations. This results in a sequence of 9 graphs of observation in our setting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the data (run only the first time to create data files)\n",
    "merged_seasons = np.array_split(all_seasons,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation of the data as a sequence of matrices of scores and adjacency matrices\n",
    "\n",
    "N = len(teams) # Number of teams\n",
    "T = len(merged_seasons) # Number of graphs\n",
    "A = np.zeros((T,N,N)) # Adjacency matrix\n",
    "Y = np.zeros((T,N,N)) # Observation matrix\n",
    "\n",
    "for i,d in enumerate(merged_seasons):\n",
    "    l_y = []\n",
    "    l_a = []\n",
    "    for s in d:\n",
    "        df = data.loc[data.Season == s] # Data contained in the i-th graph\n",
    "        for j in df.Week.unique():\n",
    "            df2 = df.loc[df.Week == j]\n",
    "            y,a = rdata.get_single_round_matrix(df2,teams) # Get scores and games played during Week j\n",
    "            l_y.append(y.to_numpy()) # List of scores\n",
    "            l_a.append(a.to_numpy()) # List of games\n",
    "            \n",
    "    Y[i,:,:] = np.mean(l_y,axis=0) # Average scores between each pair of teams connected in the i-th graph\n",
    "    A[i,:,:] = np.mean(l_a,axis=0) # Adjacency matric of the i-th graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "import pickle\n",
    "with open('epl_data/Y_merged.txt','wb') as y:\n",
    "    pickle.dump(Y,y)\n",
    "with open('epl_data/A_merged.txt','wb') as a:\n",
    "    pickle.dump(A,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been prepared, this part of the code can be run directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the prepared data\n",
    "\n",
    "import pickle\n",
    "with open('epl_data/Y_merged.txt','rb') as y:\n",
    "    Y = pickle.load(y)\n",
    "with open('epl_data/A_merged.txt','rb') as a:\n",
    "    A = pickle.load(a)\n",
    "with open('epl_data/teams.txt','rb') as t:\n",
    "    teams = pickle.load(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of graphs and of teams\n",
    "\n",
    "T,N = np.shape(Y)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check before using our method, we verify that the union of all the data form a connected graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check connectivity\n",
    "import graph_module as graph\n",
    "\n",
    "print(graph.connected(sum(A))) # Connectivity of the union graph\n",
    "\n",
    "for t in range(T):\n",
    "    print(graph.connected(A[t,:,:])) # Individual connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, individual graphs are not connected because of promotion/relegation system of EPL but the union of all the graphs is connected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of optimal hyper parameters by cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run two cross-validations procedures in order to get optimal values for the hyper parameter $\\lambda$ and $\\tau$. The criteria for these procedures are the MSE and the mean number of upsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loocv_module as cv\n",
    "import tools_module as tools\n",
    "import smoothness_module as smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "num_loocv = 40 # Number of runs for cross-validation\n",
    "lambda_list = np.linspace(0,100,50) # Candidates for lambda\n",
    "tau_list = np.linspace(1e-6,50,50) # Candidates for tau\n",
    "\n",
    "# Smoothness parameters\n",
    "E = smooth.penalty_E(N,T) # used in the DLS method\n",
    "eigs_E,V_E = smooth.eigs_E(N,T) # used in the DProj method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation with the number of upsets criterion\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Analysis with the DLS method\n",
    "lam_up,z_up_dls = cv.cv_dls_up(Y,A,E,lambda_list,num_loocv)\n",
    "\n",
    "# Analysis with the DProj method\n",
    "tau_up,z_up_dproj = cv.cv_dproj_up(Y,A,V_E,eigs_E,tau_list,num_loocv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation with the MSE criterion\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Analysis with the DLS method\n",
    "lam_mse,z_mse_dls = cv.cv_dls_mse(Y,A,E,lambda_list,num_loocv)\n",
    "\n",
    "# Analysis with the DProj method\n",
    "tau_mse,z_mse_dproj = cv.cv_dproj_mse(Y,A,V_E,eigs_E,tau_list,num_loocv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation procedures give estimators for the optimal value of hyperparameters in DLS and DProj method. Let us compute the naive LS estimator for the sake of comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computation of the LS estimator\n",
    "\n",
    "Y_vec = tools.obs_transync(Y,A) # Vectorize the observations\n",
    "Q = graph.diag_incidence(A)\n",
    "Lv = Q@Q.T # Laplacian matrix\n",
    "z_ls = scipy.sparse.linalg.lsqr(Lv,Q@Y_vec)[0] # LS estimator\n",
    "z_ls = z_ls.reshape((T,N),order='F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compute the error criterion for each estimator using the observations as ground truth. \n",
    "For estimators obtained by cross-validation with the Upsets criterion, we compute the mean number of upsets with respect to the observations.\n",
    "For estimators obtained by cross-validation with the MSE criterion, we compute the MSE with respect to the observed score differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Number of upsets \n",
    "upsets_ls = rdata.get_mean_nb_upsets(Y,A,z_ls)\n",
    "upsets_dls = rdata.get_mean_nb_upsets(Y,A,z_up_dls)\n",
    "upsets_dproj = rdata.get_mean_nb_upsets(Y,A,z_up_dproj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE\n",
    "MSE_ls = rdata.get_mse(Y,A,z_ls)\n",
    "MSE_dls = rdata.get_mse(Y,A,z_mse_dls)\n",
    "MSE_dproj = rdata.get_mse(Y,A,z_mse_dproj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(upsets_ls,upsets_dls,upsets_dproj)\n",
    "print(MSE_ls,MSE_dls,MSE_dproj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check : smoothness of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our analysis rely on a supposed smoothness of the data. Let us check that this dataset fits this criteria by defining a ground truth vector from the observations and plot its evolution for some teams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the ground truth vector $z^*$ such that \n",
    "$$z^*_{t,i} = \\frac{1}{N_{t,i}} \\sum_{j \\in N_{t,i}} y_{ij}(t)$$\n",
    "where $N_{t,i}$ denotes the set of neighbours of $i$ at time $t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ground truth\n",
    "z_star = np.zeros((T,N))\n",
    "for t in range(T):\n",
    "    for i in range(N):\n",
    "        Nti = np.sum(A[t,i,:]) # Number of games played by team i at time t\n",
    "        if Nti != 0:\n",
    "            z_star[t,i] = np.sum(Y[t,i,:])/Nti\n",
    "        else:\n",
    "            z_star[t,i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the evolution of teams that played at all times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select teams that played at all times\n",
    "l_teams = []\n",
    "for i in range(N):\n",
    "    if np.all(z_star[:,i] != 0):\n",
    "        l_teams.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for 5 teams that played at all times\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "for i in l_teams[:5]:\n",
    "    ax.plot([all_seasons[2*i] for i in range(9) ],z_star[:,i],label=teams[i])\n",
    "    \n",
    "ax.set_ylabel('Ground truth $z^{*,emp}_{t,i}$')\n",
    "ax.set_xlabel('Seasons')\n",
    "ax.xaxis.set_ticks([all_seasons[2*i] for i in range(9) ])\n",
    "ax.set_xticklabels([all_seasons[2*i] for i in range(9) ])\n",
    "\n",
    "ax.set_title('Evolution of the strength of the teams')\n",
    "plt.legend(ncol=2,loc='lower right',frameon =False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
